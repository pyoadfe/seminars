\documentclass[fontsize=12pt, paper=a4]{article}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amssymb}
\usepackage{amsmath}

\renewcommand{\vec}[1]{\mathbf{#1}}
\def\x{\vec{x}}
\def\y{\vec{y}}
\def\f{\vec{f}}
\def\c{\vec{c}}
\def\vnabla{\vec{\nabla}}
\def\d{\mathrm{d}}
\def\m{\x_*}

\begin{document}



\section{Линейный МНК}
Задача линейного МНК (метода наименьших квадратов) в терминах линейной алгебры записывается следующим образом: требуется найти вектор параметров $\m$, минимизирующий функцию $\Phi(\x)$ для данной матрицы $A$ и вектора $\y$:
$$
\Phi(\vec{x}) = \dfrac12 ||\y - A\x||^2,
$$
где $||...||$ обозначает норму, а двойка в знаменателе введена для удобства вычислений, как будет видно вскоре она пропадает при дифференцировании этого выражения.

В случае физического эксперимента, $\y$ — это результаты измерений, а $A\x$ — это предсказание линейной модели для наших измерений.
Обратите внимание, что здесь $\x$ — это неизвестные параметры модели, которые мы хотим найти из эксперимента.
С другой стороны матрица $A$ известна и определяется параметрами эксперимента.

Прировняв производную от $\Phi(x)$ по компонентам $\x$ к нулю мы получим систему линейных уравнений, решение которой даёт искомые значения $\x$:
\begin{equation}
0 = \dfrac{\partial \Phi(x)}{\partial \x_i} = A_{ji} A_{jk} x_k - A_{ji} y_j.
\end{equation}
Решение этой системы:
\begin{equation}
\m = (A^TA)^{-1} A^T \y.
\end{equation}

\section{Нелинейный МНК}
Задача нелинейного МНК ставится похожим образом, но в этот раз линейная комбинация  $A\x$ заменяется на векторную функцию $\f(\x)$:
\begin{equation}
\Phi(\x) = \dfrac12 ||\y - \f(\x)||^2 \label{eq.def_phi}
\end{equation}
Снова найдём производную этого выражения:
\begin{eqnarray}
&\dfrac{\partial \Phi(x)}{\partial x_i} = \dfrac{\partial f_j}{\partial x_i} (f_j - \y_j), \\
&\vnabla \Phi(x) = J^T (\f(\x) - \y), \label{eq.nablaPhi}
\end{eqnarray}
где якобиан $J_{ij}(\x) = \partial f_i / \partial x_j$ — известные значения производной функции $\f$ в точке $\x$.

\section{Градиентный спуск}
Можно предположить, что направление, противоположное градиенту $\vnabla \Phi(\x)$, задаёт направление, в котором стоит искать минимум функции $\Phi(\x)$.
Это даёт нам следующий способ поиска $\m$.
Пусть начальное приближение решения — это $\x^{(0)}$, тогда каждое следующее приближение $\x^{(k)}$ может быть получено следующим образом:
\begin{equation}
\x^{(k)} = \x^{(k-1)} - \dfrac{\vnabla{\Phi(\x^{(k-1)})}}{||\vnabla{\Phi(\x^{(k-1)})}||} \times s^{(k)},
\end{equation}
где $s^{(k)}$ — размер шага, его выбор может зависеть как от $k$ (обычно шаг уменьшают в процессе итераций), так и от длины $||\vnabla{\Phi(\x^{(k-1)})}||$.


\section{Метод Гаусса---Ньютона}
В некотором смысле, скалярное произведение $(\vnabla\Phi(\x), \x)$ со значением градиента $\nabla\Phi(\x)$, даваемым уравнением~\eqref{eq.nablaPhi}, является линейной аппроксимацией функции $\Phi(\x)$ в точке $\x$.
У такой линейной функции минимум не определён, поэтому распространенны методы оптимизации, в которых на каждом итерационном шаге $k$ функция $\Phi(\x^{(k-1)})$ аппроксимируется квадратичной функцией, у которой можно найти минимум и использовать его в качестве следующего приближения решения $x^{(k)}$.
Построим один из возможных вариантов такой квадратичной аппроксимации.

Разложим функцию $\f(\x^{k})$ в ряд Тейлора вокруг точки $\x^{(k-1)}$:
\begin{equation}
\f(\x^{(k)}) = \f(\x^{(k-1)}) - J(\x^{(k-1)}) (\x^{(k)} - \x^{(k-1)}) + o(|\x^{(k)} - \x^{(k-1)}|).
\end{equation}
Введём обозначение $\Delta \x \equiv \x^{(k)} - \x^{(k-1)}$ и подставим полученное выражение в определение $\Phi(\x)$~\eqref{eq.def_phi}, пренебрегая $o(|\Delta\x|)$:
\begin{equation} \label{eq.Phi_GN}
\begin{split}
\Phi(\x^{(k)}) &\simeq \dfrac12 ||\y - \f(\x^{(k-1)}) + J(\x^{(k-1)}) \Delta\x||^2 \\
 &= \dfrac12 ||\y - \f||^2 + (J^T(\y - \f), \Delta\x) + \dfrac12 (J^TJ\Delta\x, \Delta\x).
\end{split}
\end{equation}
Перед нами квадратичная форма относительно вектора $\Delta\x$.
Так как $J^TJ$ положительно определена (докажите сами), то эта квадратичная форма имеет минимум.
Этот минимум может быть найден по следующей формуле:
\begin{equation}
J^TJ \Delta\m = J^T (\y - \f). \label{eq.Gauss_Newton}
\end{equation}
На практике, значение $\Delta\m$ часто умножают на константу $K: 0 < K \leq 1$:
\begin{equation}
\x^{(k)} = \x^{(k-1)} + K \Delta\m.
\end{equation}

Обратим внимание, что формула~\eqref{eq.Phi_GN} похожа на разложение $\Phi(\x)$ в ряд Тейлора до второго порядка.
Однако, в ряд Тейлора входит дополнительное слагаемое, пропорциональное вторым производным функции $\f(\x)$.
Если функция $\f(\x)$ сильно нелинейна, то при полном разложении в ряд Тейлора $\Phi(\x)$ до второго порядка малости может быть нарушено условие положительной определенности квадратичной формы.


\section{Метод Левенберга---Марквардта}
Квадратичное представление метода Гаусса---Ньютона~\eqref{eq.Phi_GN} формально справедливо лишь в малой области вокруг точки разложения $x^{(k-1)}$.
Это значит, что предсказываемое квадратичной формой значение $\Phi(x^{(k)})$ может сильно отличаться от реального значения при больших $\Delta\x$.
Таким образом, сложное поведение функции, например седловина или резкое изменение производных, может оказаться серьёзным препятствием к сходимости.
Одним из вариантов решения проблемы является модификация уравнения ~\eqref{eq.Gauss_Newton} следующим образом:
\begin{equation}
J^TJ \Delta\m + \lambda^{(k)} I \Delta\m = J^T (\y -\f), \label{eq.LM}
\end{equation}
где $I$ --- единичная матрица, а $\lambda^{(k)}$ --- безразмерный положительный коэффициент, который изменяется согласно определённому алгоритму.
Видно, что величина $\Delta$ монотонно убывает с $\lambda$, что позволяет регулировать шаг правильно подбирая этот коэффициент.
Отметим, что это уравнение совпадает с уравнением для поиска условного экстремума в области $||\Delta\m|| < \Delta(\lambda)$.

В начале итераций выбирается начальное значение $\lambda^{(0)} > 0$, обычно берётся значение меньше единицы.
Затем, на каждом итерационном шаге $k$ рассчитывается два значения $\Phi$ — для $\Delta\x$, получаемого при $\lambda = \lambda^{(k-1)}$ и $\lambda = \lambda^{(k-1)} / \nu$, где $\nu > 1$ --- постоянный безразмерный коэффициент.
Обозначим соотвествующие значения $\Phi$ как $\Phi( \lambda^{(k-1)})$ и $\Phi(\lambda^{(k-1)} / \nu)$, а значение $\Phi$ на предыдущем итерационном шаге $k-1$ как $\Phi^{(k-1)}$.
Тогда значение $\x^{(k)} = x^{(k-1)} + \Delta\x$ ищется из решения~\eqref{eq.LM}, причём коэффициент $\lambda^{(k)}$ выбирается согласно следующему алгоритму:

\begin{enumerate}
 \item Если $\Phi(\lambda^{(k-1)} / \nu) \leq \Phi$ значит б\'{о}льший шаг улучшил приближение, и $\lambda^{(k)} = \lambda^{(k-1)} / \nu$
 \item Если $\Phi(\lambda^{(k-1)} / \nu) > \Phi$ и $\Phi(\lambda^{(k-1)}) \leq \Phi$, то старый шаг дал лучший результат, и $\lambda^{(k)} = \lambda^{(k-1)}$
 \item Если $\Phi(\lambda^{(k-1)} / \nu) > \Phi$ и $\Phi(\lambda^{(k-1)}) > \Phi(\lambda^{(k-1)})$, то результат ухудшился при обоих размерах шага и следует увеличивать $\lambda$ в $\hat\nu$ (это число не обязательно равно $\nu$) столько $w$ раз, сколько нужно чтоб $\Phi(\lambda^{(k-1)} \hat\nu^w) \leq \Phi$.
  После этого $\lambda^{(k)} = \lambda^{(k-1)} \hat\nu^w$.
\end{enumerate}

Предложенный метод называется методом Левенберга---Марквардта и является эвристическим.
Это значит, что можно придумать много правил по которым модифицируется $\lambda$, например, её увеличение в шаге 3 может производиться аддитивно, а не мультипликативно.

\section{Метод сопряженных градиентов}
Метод Левенберга---Марквардта, как и подобные ему методы, требует нахождения минимума квадратичной функции на каждом итерационном шаге.
Если размерность решаемой задачи велика ($n \gtrsim 20-30$), то нахождение минимума квадратичной формы типа~\eqref{eq.Phi_GN} с помощью обращения матрицы является вычислительно сложной задачей и требует использования $O(n^2)$ памяти для хранения различных матриц.
Метод сопряженных градиентов позволяет решить эту задачу эффективнее.

Пусть дана квадратичная функция $Q$:
\begin{equation}
Q(\x) = \dfrac12 (\x, H \x) + (\c, \x),
\end{equation}
где $H$ и $\c$ --- заданная положительно определенная матрица и заданный вектор.

Для нахождения минимума этой функции можно воспользоваться итерационным алгоритмом, состоящим из количества шагов $n$ равного размерности $\x$.
На первом шаге выбирается произвольное значение $\x^{(0)}$, а также задаются значения векторов $\vec{g}^{(0)} = H\x^{(0)} + c$ и $\vec{p}^{(0)} = - \vec{g}^{(0)}$.
Затем для всех $i$ от 0 до $n-1$ выполняется:
\begin{equation}
\begin{split}
 \alpha^{(i)} &= \dfrac{||\vec{g}^{(i)}||^2}{(p^{(i)}, H p^{(i)})},\\
 \x^{(i+1)} &= x^{(i)} + \alpha^{(i)} \vec{p}^{(i)},\\
 \vec{g}^{(i+1)} &= \vec{g}^{(i)} + \alpha^{(i)} H \vec{p}^{(i)},\\
 \beta^{(i)} &= \dfrac{||\vec{g}^{(i+1)}||^2}{||\vec{g}^{(i)}||^2},\\
 \vec{p}^{(i+1)} &= -\vec{g}^{(i+1)} + \beta^{(i)} \vec{p}^{(i)}.
\end{split}
\end{equation}

Обратим внимание на особенности этого метода:
\begin{itemize}
 \item Этот метод основан на построении крыловских подпространств и является самым быстрым известным методом нахождения минимума $Q(\x)$.
 \item Прост в реализации.
 \item Метод сходится к точному решению.
 \item Каждая последовательная итерация метода улучшает приближение находимое решение, поэтому выполнение метода можно прервать на шаге $i < n-1$ для получение приближенного решения.
 \item С другой стороны, из-за ошибок округления на практике принято использовать $2n$ шагов алгоритма для достижения точного решения.
 \item Если значения $H$ заданы программно, то требуется всего $O(n)$ памяти для реализации алгоритма. Например, в случае решения задачи оптимизации, $H$ может быть матрицей Гессе или $J^TJ$ и значения компонент матрицы могут быть вычислены программно.
 \item Этот алгоритм возможно видоизменить для работы с не положительно определенными $H$.
\end{itemize}

Отметим, что метод сопряженных градиентов используется и для решения задачи линейного МНК, если её размерность велика.

\end{document}
